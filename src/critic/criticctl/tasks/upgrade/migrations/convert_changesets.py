# -*- mode: python; encoding: utf-8 -*-
#
# Copyright 2017 the Critic contributors, Opera Software ASA
#
# Licensed under the Apache License, Version 2.0 (the "License"); you may not
# use this file except in compliance with the License.  You may obtain a copy of
# the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the
# License for the specific language governing permissions and limitations under
# the License.

import asyncio
import logging
import multiprocessing
import os
import subprocess
from collections import defaultdict

logger = logging.getLogger(__name__)

from critic import base
from critic import gitaccess

index = 1
title = "Convert legacy changeset database tables."
scope = {"database"}


def prepare_for_updates(schema_helper):
    # Create some indexes that greatly speed up the deletion of changesets.
    schema_helper.create_index(
        "CREATE INDEX reviewchangesets_changeset"
        "          ON reviewchangesets (changeset)"
    )
    schema_helper.create_index(
        "CREATE INDEX reviewfiles_changeset_file"
        "          ON reviewfiles (changeset, file)"
    )

    # Delete all changesets not connected to reviews. This makes things a lot
    # easier. In particular, the new |changesets.repository| column needs to be
    # inferred from somewhere, and if we only have changesets connected with
    # reviews, we can infer the repository from the connected review's branch.
    logger.debug("deleting non-essential changesets")
    schema_helper.execute(
        """DELETE
             FROM changesets
            WHERE id NOT IN (SELECT changeset FROM reviewchangesets)"""
    )
    schema_helper.commit()

    schema_helper.rename_table("changesets", "legacy_changesets")

    logger.debug("dropping indexes")
    schema_helper.drop_index("reviewchangesets_changeset")
    schema_helper.drop_index("reviewfiles_changeset_file")
    schema_helper.drop_index("changesets_child")
    schema_helper.drop_index("fileversions_old_sha1")
    schema_helper.drop_index("fileversions_new_sha1")

    # Drop some constraints that reference the old tables.
    logger.debug("dropping constraints")
    schema_helper.drop_constraint("reviewfiles_changeset_fkey1")
    schema_helper.drop_constraint("reviewfiles_review_fkey1")
    schema_helper.drop_constraint("reviewchangesets_pkey")

    # Update the |FOREIGN KEY (file) REFERENCES reviewfiles| constraint to have
    # |ON DELETE CASCADE|, since we will be deleting rows from |reviewfiles|
    # later.
    schema_helper.drop_constraint("reviewfilechanges_file_fkey")
    schema_helper.add_constraint(
        "reviewfilechanges",
        "reviewfilechanges_file_fkey",
        "FOREIGN KEY (file) REFERENCES reviewfiles ON DELETE CASCADE",
    )

    # Delete everything in the |codecontexts| table. All syntax highlighting
    # will be regenerated (on demand) anyway, so it's pointless to keep. We will
    # also add a |language| column to it, which we wouldn't know what to set to
    # for existing rows.
    schema_helper.drop_index("codecontexts_sha1_first_last")
    schema_helper.execute("DELETE FROM codecontexts")
    schema_helper.commit()


def create_new_objects(schema_helper):
    # New definitions from changesets.sql:
    schema_helper.update(
        """

-- Structure difference:
--   One row per changeset
CREATE TABLE changesets (
  id SERIAL PRIMARY KEY,

  -- Repository in which the difference is calculated.
  --
  -- The same pair of commits could be compared in multiple repositories (with
  -- necessarily identical results) but a reference is required to actually
  -- process the difference.  Optimizing for this seems unnecessary.
  repository INTEGER NOT NULL REFERENCES repositories ON DELETE CASCADE,
  -- "New"/"right-hand side" commit.
  to_commit INTEGER NOT NULL REFERENCES commits ON DELETE CASCADE,
  -- "Old"/"left-hand side" commit.  NULL means difference against the empty
  -- tree, e.g. because |to_commit| is a root commit.
  from_commit INTEGER REFERENCES commits ON DELETE CASCADE,
  -- If non-NULL, this is a "merge filtered" changeset, meaning the set of
  -- changed files recorded in changesetfiles is filtered to only include files
  -- changed on two or more sides of the merge.
  --
  -- This will only be set on changesets where either |to_commit| is the merge
  -- (same as |for_merge|) and |from_commit| is one of the parents, or
  -- changesets where |to_commit| is one of the parents and |from_commit| is the
  -- merge base.
  for_merge INTEGER REFERENCES commits ON DELETE CASCADE,
  -- If true, the |from_commit| is the result of replaying a merge or rebase,
  -- and may have conflict markers generated by Git checked in, for reference.
  -- This may trigger different analysis choices, and enables special syntax
  -- highlighting of the conflict marker lines.
  is_replay BOOLEAN NOT NULL DEFAULT FALSE,

  -- Set to TRUE when the structure difference has been preliminarily
  -- calculated.
  processed BOOLEAN NOT NULL DEFAULT FALSE,
  -- Set to TRUE when the structure difference has been fully calculated.  At
  -- this time, all rows in changesetfiles make up the structure difference will
  -- have been inserted.  Even before that, some of those rows may have been
  -- insterted.
  complete BOOLEAN NOT NULL DEFAULT FALSE
);

-- Index for lookup speed and to ensure uniqueness of non-partial changesets.
CREATE UNIQUE INDEX changesets_regular
  ON changesets (repository, to_commit, from_commit)
  WHERE for_merge IS NULL;

-- Index for lookup speed of partial changesets.
CREATE UNIQUE INDEX changesets_for_merge
  ON changesets (repository, to_commit, from_commit, for_merge)
  WHERE for_merge IS NOT NULL;

-- General:
--   One row per error during changeset processing
--
-- Rows in this table disable further attempts to perform the failed job until
-- cleared, to avoid endless spamming.  They also serve to indicate to clients
-- that there's a problem and that they should not expect a finished result any
-- time soon.
CREATE TABLE changeseterrors (
  changeset INTEGER REFERENCES changesets ON DELETE CASCADE,

  -- A job key is a Python tuple containing the class name of the job and a
  -- small number of integers and in some cases a SHA-1 sum.  This column
  -- contains a string produced converting this tuple to JSON.
  job_key VARCHAR(256),

  -- Fatality.  A fatal error means the content difference is not usable.  A
  -- non-fatal error will simply lead to reduced functionality.
  fatal BOOLEAN NOT NULL,

  -- A Python traceback.
  traceback TEXT NOT NULL,

  PRIMARY KEY (changeset, job_key)
);

-- Structure difference:
--   One row per added/removed/modified file.
--
-- Matched by one row in changesetfiledifferences iff content difference has
-- been calculated.
--
-- Note that added/removed/modified sub-module entries are represented as files
-- in this context, but the SHA-1 is of a commit in the sub-module repository
-- rather than a blob in "this" repository.
CREATE TABLE changesetfiles (
  changeset INTEGER REFERENCES changesets ON DELETE CASCADE,
  file INTEGER REFERENCES files,

  -- Old SHA-1.  NULL means "non-existing", i.e. file was added.
  old_sha1 CHAR(40),
  -- Old file mode.  NULL means "non-existing", i.e. file was added.
  old_mode INTEGER,
  -- New SHA-1.  NULL means "non-existing", i.e. file was removed.
  new_sha1 CHAR(40),
  -- Old file mode.  NULL means "non-existing", i.e. file was removed.
  new_mode INTEGER,

  PRIMARY KEY (changeset, file)
);

CREATE TABLE highlightlanguages (
  id SERIAL PRIMARY KEY,
  label VARCHAR(64) NOT NULL UNIQUE
);

-- Syntax highlighting request:
--   One row per file version that needs to be highlighted in a changeset.
CREATE TABLE highlightfiles (
  id SERIAL PRIMARY KEY,

  repository INTEGER NOT NULL,
  sha1 CHAR(40) NOT NULL,
  language INTEGER NOT NULL,
  conflicts BOOLEAN NOT NULL,

  -- Whether this file is currently highlighted; IOW, whether lines exists in
  -- the |highlightlines| table.
  highlighted BOOLEAN DEFAULT FALSE,
  -- Whether this file should be highlighted, when it isn't.
  requested BOOLEAN DEFAULT TRUE,

  UNIQUE (repository, sha1, language, conflicts),
  FOREIGN KEY (repository)
    REFERENCES repositories ON DELETE CASCADE,
  FOREIGN KEY (language)
    REFERENCES highlightlanguages ON DELETE SET NULL,

  -- It is not valid for |requested| to be TRUE if |highlighted| is also TRUE.
  -- Both can be FALSE, or one of them TRUE, but never both.
  CHECK (NOT requested OR NOT highlighted)
);
CREATE INDEX highlightfiles_sha1_language
          ON highlightfiles (sha1, language);

CREATE TABLE highlightlines (
  file INTEGER NOT NULL,
  line INTEGER NOT NULL,
  data BYTEA NOT NULL,

  PRIMARY KEY (file, line),
  FOREIGN KEY (file)
    REFERENCES highlightfiles ON DELETE CASCADE
);

-- Content difference:
--   One row per changeset
--
-- Matches one row in changesets (the structure difference).
--
-- Inserting a row into this table represents a request for the content
-- difference to be calculated.  Once calculated, the |complete| column is set
-- to TRUE.
--
-- The |requested| column contains a timestamp that is used to determine when
-- the content difference should be garbage collected.
CREATE TABLE changesetcontentdifferences (
  changeset INTEGER PRIMARY KEY REFERENCES changesets ON DELETE CASCADE,

  -- Timestamp of request or last use.  Used to determine when it makes sense to
  -- garbage collect (i.e. delete) the content difference to save space.
  requested TIMESTAMP NOT NULL DEFAULT NOW(),
  -- Set to TRUE when the content difference has been fully calculated.  At this
  -- time, all rows in changesetfiledifferences and changesetchangedlines that
  -- make up the content difference will have been inserted.  Even before that,
  -- some of those rows may have been insterted.
  complete BOOLEAN NOT NULL DEFAULT FALSE
);

-- Content difference:
--   One row per added/removed/modified regular file or symbolic link.
--
-- Matches one row in changesetfiles (the structure difference).
--
-- Iff either old_is_binary or new_is_binary is FALSE (i.e. neither NULL nor
-- TRUE) then a row will be matched by at least one row in
-- changesetchangedlines, once the content difference has been fully processed.
CREATE TABLE changesetfiledifferences (
  changeset INTEGER REFERENCES changesetcontentdifferences ON DELETE CASCADE,
  file INTEGER,

  -- TRUE if the file versions will be compared but has not been compared yet.
  -- If the file was either added or removed, or modified but at least one of
  -- the versions is binary, this value is set to FALSE initially, otherwise it
  -- is set to FALSE when corresponding rows have been inserted into the
  -- |changesetchangedlines| table.
  comparison_pending BOOLEAN,

  -- TRUE if old version is binary. NULL means "non-existing", i.e. file was
  -- added.
  old_is_binary BOOLEAN,
  -- Number of lines in the old version. NULL means "non-existing" or that the
  -- file is binary (use `old_is_binary` to differentiate.)
  old_length INTEGER,
  -- TRUE if the old version has a trailing line-break. NULL means
  -- "non-existing", i.e. file was added.
  old_linebreak BOOLEAN,
  -- The syntax highlighting record for the old version of the file. NULL if the
  -- file was added.
  old_highlightfile INTEGER,
  -- TRUE if new version is binary. NULL means "non-existing", i.e. file was
  -- removed.
  new_is_binary BOOLEAN,
  -- Number of lines in the new version. NULL means "non-existing" or that the
  -- file is binary (use `new_is_binary` to differentiate.)
  new_length INTEGER,
  -- TRUE if the new version has a trailing line-break. NULL means
  -- "non-existing", i.e. file was added.
  new_linebreak BOOLEAN,
  -- The syntax highlighting record for the new version of the file. NULL if the
  -- file was removed.
  new_highlightfile INTEGER,

  PRIMARY KEY (changeset, file),
  FOREIGN KEY (changeset)
    REFERENCES changesetcontentdifferences ON DELETE CASCADE,
  FOREIGN KEY (changeset, file)
    REFERENCES changesetfiles,
  FOREIGN KEY (old_highlightfile)
    REFERENCES highlightfiles,
  FOREIGN KEY (new_highlightfile)
    REFERENCES highlightfiles
);
CREATE INDEX changesetfiledifferences_old_highlightfile
          ON changesetfiledifferences (old_highlightfile);
CREATE INDEX changesetfiledifferences_new_highlightfile
          ON changesetfiledifferences (new_highlightfile);

CREATE VIEW changesetmodifiedregularfiles
  AS SELECT changeset, file, old_sha1, old_mode, new_sha1, new_mode
       FROM changesetfiles
       JOIN changesetfiledifferences USING (changeset, file)
      WHERE old_sha1 IS NOT NULL
        AND new_sha1 IS NOT NULL
        AND old_sha1 != new_sha1
        AND ((old_mode | new_mode) & 261632) = 32768
        AND NOT old_is_binary
        AND NOT new_is_binary;

-- Content difference:
--   Zero or more rows per added/removed/modified regular file.
--
-- Each row represents one block of lines in the old version being replaced by
-- one block of lines in the new version.  Either of these blocks can be empty,
-- meaning lines were only added (delete_count=0) or removed (insert_count=0).
--
-- Added or removed files are represented as a single block of changed lines
-- that adds or removes all lines at offset zero.  This simply serves to record
-- the number of lines in the added or removed file.
--
-- Binary files being added, removed or modified is represented by no rows.  A
-- file being modified so that it becomes or stops being binary is represented
-- by a single row, as if the non-binary version had been removed or added.
--
-- A zero-length file is represented as a non-binary file that has zero lines.
--
-- A symbolic link is represented as a text file containing a single line.
--
-- A sub-module file ("git link") is represented as a binary file.
CREATE TABLE changesetchangedlines (
  changeset INTEGER,
  file INTEGER,

  -- Zero-based block index.
  "index" INTEGER NOT NULL,

  -- Offset (number of lines) from preceding block of changed lines, or from the
  -- beginning of the file if |index=0|. Can only be zero if |index=0|.
  "offset" INTEGER NOT NULL,

  -- Number of deleted lines, i.e. lines that existed in the old version but
  -- don't exist in the new version.
  delete_count INTEGER NOT NULL,
  -- Length of the block in the old version.  This is |delete_count| plus any
  -- lines that were included despite being identical.
  delete_length INTEGER NOT NULL,

  -- Number of inserted lines, i.e. lines that exist in the new version but
  -- didn't exist in the old version.
  insert_count INTEGER NOT NULL,
  -- Length of the block in the new version.  This is |insert_count| plus any
  -- lines that were included despite being identical.
  insert_length INTEGER NOT NULL,

  -- Content difference analysis.  NULL if not processed yet.  Always set to
  -- non-NULL by processing; set to empty string if there's no relevant result
  -- (e.g. because lines were only added or removed.)
  analysis TEXT,

  -- Technically, |delete_offset| and |insert_offset| are independently unique
  -- per |changeset|+|file| pair, since we never have adjacent blocks of changed
  -- lines.  To optimize somewhat, and still detect duplicated insertion of the
  -- same file difference, have a single constraint (which also serves as a
  -- useful implicit index.)
  PRIMARY KEY (changeset, file, index),

  FOREIGN KEY (changeset, file)
    REFERENCES changesetfiledifferences ON DELETE CASCADE,

  -- Implementation detail: blocks of changed lines that represent a pure
  -- removal of lines or insertion of lines, i.e. where either |delete_count| or
  -- |insert_count| is zero, has no relevant analysis.  Make sure we always set
  -- the analysis to the empty string on insertion, rather than inserting NULL,
  -- thus "scheduling" a pointless analysis of the block.
  CHECK (analysis IS NOT NULL OR (delete_count!=0 AND insert_count!=0))
);

-- Syntax highlighting request:
--   One row per changeset with files that need to be highlighted.
CREATE TABLE changesethighlightrequests (
  -- Reference the content difference.  When the content difference is garbage
  -- collected, so will the syntax highlighted copies of the files be, unless
  -- they are referenced via other changesets as well.
  changeset INTEGER,

  -- Set to TRUE when all changed files in the changeset have been evaluated
  -- (the appropriate language to syntax highlight as has been calculated,)
  -- individual syntax highlight requests for each file that should be syntax
  -- highlighted have been inserted into |highlightfiles|.
  evaluated BOOLEAN NOT NULL DEFAULT FALSE,

  -- If TRUE, rows inserted into |highlightfiles| will have their |requested|
  -- column set to TRUE as well. Updating this column later has no effect; to
  -- re-request deleted syntax highlighting, the |highlightfiles.requested|
  -- column should be updated instead.
  requested BOOLEAN NOT NULL DEFAULT TRUE,

  PRIMARY KEY (changeset),
  FOREIGN KEY (changeset)
    REFERENCES changesetcontentdifferences ON DELETE CASCADE
);

-- Custom syntax highlighting request:
--   One row per file version that needs to be highlighted.
--
-- The actual files to highlight are stored in |highlightfiles|.
CREATE TABLE customhighlightrequests (
  id SERIAL PRIMARY KEY,

  file INTEGER NOT NULL,

  -- Time of last access of the highlighted file. This is used to determine when
  -- to drop the highlight data to save space.
  last_access TIMESTAMP NOT NULL DEFAULT NOW(),

  FOREIGN KEY (file)
    REFERENCES highlightfiles ON DELETE CASCADE
);

CREATE INDEX customhighlightrequests_file
          ON customhighlightrequests (file);

-- Merge replay request:
--   One row per merge commit to replay.
CREATE TABLE mergereplayrequests (
  repository INTEGER REFERENCES repositories ON DELETE CASCADE,
  -- Original merge commit to replay.
  merge INTEGER REFERENCES commits ON DELETE CASCADE,

  -- The merge commit produced by replaying or NULL if not replayed yet.
  replay INTEGER REFERENCES commits,
  -- A Python traceback, if replaying failed.  NULL otherwise.
  traceback TEXT,

  PRIMARY KEY (repository, merge),
  CHECK (replay IS NULL OR traceback IS NULL)
);

"""
    )


def convert_data(schema_helper):
    # Create a temporary column that links a "new" changeset back to the "old"
    # one from which is was created.
    schema_helper.add_column("changesets", "legacy_changeset", "INTEGER")

    # Create a corresponding column in the old |legacy_changesets| table.
    schema_helper.add_column("legacy_changesets", "changeset", "INTEGER")

    # Transfer all remaining "direct" changesets.
    schema_helper.execute(
        """INSERT
             INTO changesets (repository, to_commit, from_commit, processed,
                              complete, legacy_changeset)
           SELECT DISTINCT reviews.repository, legacy_changesets.child,
                  legacy_changesets.parent, TRUE, TRUE, legacy_changesets.id
             FROM legacy_changesets
             JOIN reviewchangesets ON (
                    reviewchangesets.changeset=legacy_changesets.id
                  )
             JOIN reviews ON (reviews.id=reviewchangesets.review)
            WHERE legacy_changesets.type='direct'"""
    )
    schema_helper.commit()

    # Populate the |legacy_changesets.changeset| column.
    schema_helper.execute(
        """UPDATE legacy_changesets
              SET changeset=changesets.id
             FROM changesets
            WHERE changesets.legacy_changeset=legacy_changesets.id"""
    )
    schema_helper.commit()

    # All existing changesets have complete content differences as well, so just
    # insert one row for each into |changesetcontentdifferences|.
    schema_helper.execute(
        """INSERT
             INTO changesetcontentdifferences (changeset, complete)
           SELECT id, TRUE
             FROM changesets"""
    )

    # Aslo insert one row for each changeset into |changesethighlightrequests|.
    schema_helper.execute(
        """INSERT
             INTO changesethighlightrequests (changeset, evaluated, requested)
           SELECT id, FALSE, FALSE
             FROM changesets"""
    )

    def convert_from_octal(column_name):
        return "+".join(
            [
                f"{0o100000} * CAST(SUBSTRING({column_name}, 1, 1) AS INTEGER)",
                f"{0o10000} * CAST(SUBSTRING({column_name}, 2, 1) AS INTEGER)",
                f"{0o1000} * CAST(SUBSTRING({column_name}, 3, 1) AS INTEGER)",
                f"{0o100} * CAST(SUBSTRING({column_name}, 4, 1) AS INTEGER)",
                f"{0o10} * CAST(SUBSTRING({column_name}, 5, 1) AS INTEGER)",
                f"CAST(SUBSTRING({column_name}, 6, 1) AS INTEGER)",
            ]
        )

    # Transfer from |fileversions| to |changesetfiles| for handled changesets.
    # Adjustments: in the old table, an all-zero SHA-1 sum is used to represent
    #              a non-existing file version (i.e. an added or removed file),
    #              whereas in the new table this is represented as NULL instead.
    schema_helper.execute(
        f"""INSERT
              INTO changesetfiles (
                     changeset, file, old_sha1, old_mode, new_sha1, new_mode
                   )
            SELECT legacy_changesets.changeset, file,
                   NULLIF(old_sha1, '0000000000000000000000000000000000000000'),
                   {convert_from_octal("old_mode")},
                   NULLIF(new_sha1, '0000000000000000000000000000000000000000'),
                   {convert_from_octal("new_mode")}
              FROM legacy_changesets
              JOIN fileversions ON (fileversions.changeset=legacy_changesets.id)
             WHERE legacy_changesets.changeset IS NOT NULL"""
    )
    schema_helper.commit()

    # Create one row in |changesetfiledifferences| per row in |changesetfiles|.
    # The actual information stored here ("is binary" and "length") needs to be
    # calculated separately later; this information is not stored reliably in
    # the old tables.
    schema_helper.execute(
        """INSERT
             INTO changesetfiledifferences (changeset, file, comparison_pending)
           SELECT changeset, file, FALSE
             FROM changesetfiles"""
    )

    # Transfer from |chunks| to |changesetchangedlines| for handled changesets.
    # Adjustments: in the old table, the delete/insert offsets are one-based,
    #              with zero being used for the non-existing side when a file is
    #              added or removed, and for both sides for modified binary
    #              files, whereas in the new table the offsets are zero-based,
    #              with added/removed and binary files having no rows at all.
    # Also note: the new table separates "count" (number of changed lines) and
    #            "length" (number of lines in the block) to account for
    #            unchanged (low-information) lines included for improved diff
    #            analysis. The old table only stores "length". The "count" is
    #            only used to display per-file "-N/+N" information.
    cursor = schema_helper.database.cursor()
    cursor.execute(
        """SELECT legacy_changesets.changeset, file, deleteOffset, deleteCount,
                  insertOffset, insertCount, analysis
             FROM legacy_changesets
             JOIN chunks ON (chunks.changeset=legacy_changesets.id)
            WHERE legacy_changesets.changeset IS NOT NULL
              AND (chunks.deleteOffset!=0 OR chunks.insertOffset!=0)
         ORDER BY legacy_changesets.changeset, file, deleteOffset ASC"""
    )

    current_key = None
    current_index = None
    current_old_offset = None
    current_new_offset = None
    rows = []

    def flush():
        nonlocal rows
        schema_helper.executemany(
            """INSERT
                 INTO changesetchangedlines (
                        changeset, file, "index", "offset", delete_count,
                        delete_length, insert_count, insert_length, analysis
                      )
               VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)""",
            rows,
        )
        rows = []

    for (
        changeset_id,
        file_id,
        old_offset,
        old_length,
        new_offset,
        new_length,
        analysis,
    ) in cursor:
        if (changeset_id, file_id) != current_key:
            current_key = (changeset_id, file_id)
            current_index = current_old_offset = current_new_offset = 0

        assert current_index is not None

        if old_offset == 0:
            assert current_index == 0
            assert new_offset == 1
            rows.append((changeset_id, file_id, 0, 0, 0, 0, new_length, new_length, ""))
            current_index = None
        elif new_offset == 0:
            assert current_index == 0
            assert old_offset == 1
            rows.append((changeset_id, file_id, 0, 0, old_length, old_length, 0, 0, ""))
            current_index = None
        else:
            offset = (old_offset - 1) - current_old_offset
            assert offset == (new_offset - 1) - current_new_offset

            rows.append(
                (
                    changeset_id,
                    file_id,
                    current_index,
                    offset,
                    old_length,
                    old_length,
                    new_length,
                    new_length,
                    analysis or "",
                )
            )

            current_index += 1
            current_old_offset = (old_offset - 1) + old_length
            current_new_offset = (new_offset - 1) + new_length

        if len(rows) == 100000:
            flush()

    if rows:
        flush()

    schema_helper.commit()

    # schema_helper.execute(
    #     """INSERT
    #          INTO changesetchangedlines (
    #                 changeset, file, index, offset, delete_count,
    #                 delete_length, insert_offset, insert_count, insert_length,
    #                 analysis
    #               )
    #        SELECT legacy_changesets.changeset, file,
    #               deleteOffset - 1, deleteCount, deleteCount,
    #               insertOffset - 1, insertCount, insertCount,
    #               COALESCE(analysis, '')
    #          FROM legacy_changesets
    #          JOIN chunks ON (chunks.changeset=legacy_changesets.id)
    #         WHERE legacy_changesets.changeset IS NOT NULL
    #           AND chunks.deleteOffset!=0
    #           AND chunks.insertOffset!=0""")

    def convert_changeset_column(table_name, column_name="changeset"):
        schema_helper.rename_column(table_name, column_name, f"legacy_{column_name}")
        schema_helper.add_column(
            table_name, column_name, "INTEGER REFERENCES changesets"
        )
        schema_helper.execute(
            f"""UPDATE {table_name}
                   SET {column_name}=lc.changeset
                  FROM legacy_changesets AS lc
                 WHERE {table_name}.legacy_{column_name}=lc.id"""
        )
        schema_helper.commit()

    convert_changeset_column("reviewchangesets")
    convert_changeset_column("reviewfiles")

    schema_helper.add_column(
        "codecontexts", "language", "INTEGER NOT NULL REFERENCES highlightlanguages"
    )
    schema_helper.add_constraint(
        "codecontexts",
        "codecontexts_pkey",
        "PRIMARY KEY (sha1, language, first_line, last_line)",
    )


def cleanup(schema_helper):
    # Delete the |reviewchangesets| row for changesets we didn't carry over.
    # These are changesets related to merges and rebases, for which the
    # representation changed too much to simply convert. Recreating these
    # changesets and still preserving the reviewing information is difficult.
    # The loss is acceptable; both merges and rebases are (supposedly) not
    # introducing new changes, only possibly "adjusting" existing changes.
    schema_helper.execute(
        """DELETE
             FROM reviewchangesets
            WHERE changeset IS NULL"""
    )
    schema_helper.execute(
        """DELETE
             FROM reviewfiles
            WHERE changeset IS NULL"""
    )
    schema_helper.commit()

    schema_helper.alter_column("reviewchangesets", "changeset", not_null=True)
    schema_helper.alter_column("reviewfiles", "changeset", not_null=True)

    schema_helper.drop_column("reviewchangesets", "legacy_changeset")
    schema_helper.drop_column("reviewfiles", "legacy_changeset")

    schema_helper.drop_table("chunks")
    schema_helper.drop_table("fileversions")
    schema_helper.drop_table("customchangesets")
    schema_helper.drop_table("legacy_changesets")
    schema_helper.drop_column("changesets", "legacy_changeset")


async def classify_files(semaphore, git, repository_path, commit_sha1, file_paths):
    async with semaphore:
        repositories_dir = base.configuration()["paths.repositories"]
        cwd = os.path.join(repositories_dir, repository_path)

        process = await asyncio.create_subprocess_exec(
            git,
            "ls-tree",
            "-z",
            commit_sha1,
            "--",
            *file_paths,
            cwd=cwd,
            stdout=subprocess.PIPE,
        )

        result = []
        found_paths = set()

        try:
            while True:
                entry = await process.stdout.readuntil(b"\0")
                mode, _, rest = entry[:-1].partition(b" ")
                _, _, file_path = rest.rpartition(b"\t")
                file_path = file_path.decode()
                found_paths.add(file_path)
                mode = int(mode, 8)
                result.append(((repository_path, commit_sha1, file_path), (mode,)))
        except asyncio.IncompleteReadError as error:
            assert not error.partial

        await process.wait()
        assert process.returncode == 0

        missing_paths = set(file_paths) - found_paths
        assert not missing_paths, (repository_path, commit_sha1, missing_paths)

        return result


async def populate_changesetfiles(git, schema_helper):
    """Populate old_mode/new_mode columns

       In the old format, these were only set for added or removed files (and
       perhaps for file changes where the mode was actually modified.) We now
       want them set in all rows, except the for "sides" where the file does not
       exist, of course.

       The modes are fetched from the repository using `git ls-tree`."""

    cursor = schema_helper.database.cursor()

    cursor.execute(
        """SELECT changesetfiles.changeset, changesetfiles.file,
                  repositories.path, commits.sha1, files.path
             FROM changesetfiles
             JOIN changesets ON (changesets.id=changesetfiles.changeset)
             JOIN repositories ON (repositories.id=changesets.repository)
             JOIN commits ON (commits.id=changesets.from_commit)
             JOIN files ON (files.id=changesetfiles.file)
            WHERE changesetfiles.old_sha1 IS NOT NULL
              AND changesetfiles.old_mode IS NULL"""
    )

    old_files = {
        (changeset_id, file_id): (repository_path, commit_sha1, file_path)
        for changeset_id, file_id, repository_path, commit_sha1, file_path in cursor
    }

    cursor.execute(
        """SELECT changesetfiles.changeset, changesetfiles.file,
                  repositories.path, commits.sha1, files.path
             FROM changesetfiles
             JOIN changesets ON (changesets.id=changesetfiles.changeset)
             JOIN repositories ON (repositories.id=changesets.repository)
             JOIN commits ON (commits.id=changesets.to_commit)
             JOIN files ON (files.id=changesetfiles.file)
            WHERE changesetfiles.new_sha1 IS NOT NULL
              AND changesetfiles.new_mode IS NULL"""
    )

    new_files = {
        (changeset_id, file_id): (repository_path, commit_sha1, file_path)
        for changeset_id, file_id, repository_path, commit_sha1, file_path in cursor
    }

    all_files = set(old_files.values()) | set(new_files.values())

    per_repository = defaultdict(lambda: defaultdict(set))

    for repository_path, commit_sha1, file_path in all_files:
        per_repository[repository_path][commit_sha1].add(file_path)

    classify_tasks = set()
    cpu_count = multiprocessing.cpu_count()
    semaphore = asyncio.Semaphore(cpu_count * 2)

    for repository_path, per_commit in per_repository.items():
        for commit_sha1, file_paths in per_commit.items():
            file_paths = sorted(file_paths)
            while file_paths:
                classify_tasks.add(
                    asyncio.ensure_future(
                        classify_files(
                            semaphore,
                            git,
                            repository_path,
                            commit_sha1,
                            file_paths[:1024],
                        )
                    )
                )
                del file_paths[:1024]

    classify_results = {}
    total_tasks = len(classify_tasks)

    while classify_tasks:
        done, classify_tasks = await asyncio.wait(classify_tasks, timeout=1)
        for task in done:
            classify_results.update(task.result())
        logger.debug(
            "progress: %.1f %%",
            100 * ((total_tasks - len(classify_tasks)) / total_tasks),
        )

    changesetfiles_old_values = [
        classify_results[key] + (changeset_id, file_id)
        for (changeset_id, file_id), key in old_files.items()
    ]

    cursor.executemany(
        """UPDATE changesetfiles
              SET old_mode=%s
            WHERE changeset=%s
              AND file=%s""",
        changesetfiles_old_values,
    )

    changesetfiles_new_values = [
        classify_results[key] + (changeset_id, file_id)
        for (changeset_id, file_id), key in new_files.items()
    ]

    cursor.executemany(
        """UPDATE changesetfiles
              SET new_mode=%s
            WHERE changeset=%s
              AND file=%s""",
        changesetfiles_new_values,
    )

    schema_helper.commit()


async def examine_files(semaphore, git, repository_path, commit_sha1, file_paths):
    async with semaphore:
        repositories_dir = base.configuration()["paths.repositories"]
        cwd = os.path.join(repositories_dir, repository_path)

        process = await asyncio.create_subprocess_exec(
            git,
            "diff-tree",
            "-r",
            "-z",
            "--numstat",
            gitaccess.EMPTY_TREE_SHA1,
            commit_sha1,
            "--",
            *file_paths,
            cwd=cwd,
            stdout=subprocess.PIPE,
        )

        result = []
        found_paths = set()

        try:
            while True:
                entry = await process.stdout.readuntil(b"\0")
                added, removed, file_path = entry[:-1].split(b"\t", 2)
                file_path = file_path.decode()
                found_paths.add(file_path)
                assert removed in (b"0", b"-")
                is_binary = added == b"-"
                length = int(added) if not is_binary else None
                result.append(
                    ((repository_path, commit_sha1, file_path), (is_binary, length))
                )
        except asyncio.IncompleteReadError as error:
            assert not error.partial

        await process.wait()
        assert process.returncode == 0

        missing_paths = set(file_paths) - found_paths
        assert not missing_paths, (repository_path, commit_sha1, missing_paths)

        return result


async def populate_changesetfiledifferences(git, schema_helper):
    """Populate {old,new}{_is_binary,_length} columns

       In the old format, "is binary" was represented as a special single chunk
       in the |chunks| table, and "length" was not stored at all (instead it was
       calculated from the actual file contents when displaying the diff.)

       The information is calculated using `git diff-tree --numstat` comparing
       each relevant commit to the empty tree."""

    cursor = schema_helper.database.cursor()

    cursor.execute(
        """SELECT changesetfiles.changeset, changesetfiles.file,
                  repositories.path, commits.sha1, files.path
             FROM changesetfiles
             JOIN changesets ON (changesets.id=changesetfiles.changeset)
             JOIN repositories ON (repositories.id=changesets.repository)
             JOIN commits ON (commits.id=changesets.from_commit)
             JOIN files ON (files.id=changesetfiles.file)
            WHERE changesetfiles.old_sha1 IS NOT NULL"""
    )

    old_files = {
        (changeset_id, file_id): (repository_path, commit_sha1, file_path)
        for changeset_id, file_id, repository_path, commit_sha1, file_path in cursor
    }

    cursor.execute(
        """SELECT changesetfiles.changeset, changesetfiles.file,
                  repositories.path, commits.sha1, files.path
             FROM changesetfiles
             JOIN changesets ON (changesets.id=changesetfiles.changeset)
             JOIN repositories ON (repositories.id=changesets.repository)
             JOIN commits ON (commits.id=changesets.to_commit)
             JOIN files ON (files.id=changesetfiles.file)
            WHERE changesetfiles.new_sha1 IS NOT NULL"""
    )

    new_files = {
        (changeset_id, file_id): (repository_path, commit_sha1, file_path)
        for changeset_id, file_id, repository_path, commit_sha1, file_path in cursor
    }

    all_files = set(old_files.values()) | set(new_files.values())

    per_repository = defaultdict(lambda: defaultdict(set))

    for repository_path, commit_sha1, file_path in all_files:
        per_repository[repository_path][commit_sha1].add(file_path)

    examine_tasks = set()
    cpu_count = multiprocessing.cpu_count()
    semaphore = asyncio.Semaphore(cpu_count * 2)

    for repository_path, per_commit in per_repository.items():
        for commit_sha1, file_paths in per_commit.items():
            file_paths = sorted(file_paths)
            while file_paths:
                examine_tasks.add(
                    asyncio.ensure_future(
                        examine_files(
                            semaphore,
                            git,
                            repository_path,
                            commit_sha1,
                            file_paths[:1024],
                        )
                    )
                )
                del file_paths[:1024]

    examine_results = {}
    total_tasks = len(examine_tasks)

    while examine_tasks:
        done, examine_tasks = await asyncio.wait(examine_tasks, timeout=1)
        for task in done:
            examine_results.update(task.result())
        logger.debug(
            "progress: %.1f %%",
            100 * ((total_tasks - len(examine_tasks)) / total_tasks),
        )

    changesetfiledifferences_old_values = [
        examine_results[key] + (changeset_id, file_id)
        for (changeset_id, file_id), key in old_files.items()
    ]

    cursor.executemany(
        """UPDATE changesetfiledifferences
              SET old_is_binary=%s,
                  old_length=%s,
                  old_linebreak=TRUE
            WHERE changeset=%s
              AND file=%s""",
        changesetfiledifferences_old_values,
    )

    changesetfiledifferences_new_values = [
        examine_results[key] + (changeset_id, file_id)
        for (changeset_id, file_id), key in new_files.items()
    ]

    cursor.executemany(
        """UPDATE changesetfiledifferences
              SET new_is_binary=%s,
                  new_length=%s,
                  new_linebreak=TRUE
            WHERE changeset=%s
              AND file=%s""",
        changesetfiledifferences_new_values,
    )

    schema_helper.commit()


async def perform(critic, arguments):
    from . import DatabaseSchemaHelper

    from critic import gitaccess

    git = gitaccess.git()
    assert git is not None

    with DatabaseSchemaHelper(critic) as schema_helper:
        logger.debug("prepare_for_updates()")
        prepare_for_updates(schema_helper)
        logger.debug("create_new_objects()")
        create_new_objects(schema_helper)
        logger.debug("convert_data()")
        convert_data(schema_helper)
        logger.debug("cleanup()")
        cleanup(schema_helper)
        logger.debug("populate_changesetfiles()")
        await populate_changesetfiles(git, schema_helper)
        logger.debug("populate_changesetfiledifferences()")
        await populate_changesetfiledifferences(git, schema_helper)
